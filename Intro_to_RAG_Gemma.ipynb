{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyM17Uures8533DUo7YF98+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathalyAlarconT/GenAI_Workshops/blob/main/Intro_to_RAG_Gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup del Notebook"
      ],
      "metadata": {
        "id": "eBpCroEr6JVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IP Asignada a Google Colab\n",
        "!curl ipecho.net/plain\n",
        "# https://ipinfo.io/IP"
      ],
      "metadata": {
        "id": "efmJLyPaaZEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando las librerías Requeridas"
      ],
      "metadata": {
        "id": "hdd6n79L6OG4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70m9T35fHOq7"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q langchain\n",
        "!pip install google-generativeai langchain-google-genai\n",
        "!pip install chromadb pypdf2 python-dotenv\n",
        "!pip install PyPDF\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías Generales"
      ],
      "metadata": {
        "id": "Vt4wMCO06lez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "n9aR9wbcJajt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurando credenciales"
      ],
      "metadata": {
        "id": "4XZPHVVJ6v4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('SMITH_APIKEY')\n",
        "GOOGLE_API_KEY = userdata.get('GoogleAIStudio')"
      ],
      "metadata": {
        "id": "U6DoVr1AIi5b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de Folders necesarios\n",
        "\n",
        "Usaremos dos carpetas:\n",
        "- Mis Datos, almacenara los PDFs con la info propia\n",
        "- VectorDB, en esta carpeta se creará la base de datos vectorial"
      ],
      "metadata": {
        "id": "_XtYfNeg641e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/MisDatos\n",
        "!mkdir /content/VectorDB"
      ],
      "metadata": {
        "id": "VTGteWixWdgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080e48ba-a769-47f5-e933-81e308e9742c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/MisDatos’: File exists\n",
            "mkdir: cannot create directory ‘/content/VectorDB’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adiciona los archivos PDF a la carpeta Mis Datos\n",
        "\n",
        "El archivo utilizado en la demo es:\n",
        "https://www.lostiempos.com/sites/default/files/edicion_online/las_delicias_de_mi_llajta.pdf\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PYBdnn-nXLb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. INDEXING"
      ],
      "metadata": {
        "id": "s_sn_QWZOf1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Librerías necesarias"
      ],
      "metadata": {
        "id": "SMmDGkvr8f41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings"
      ],
      "metadata": {
        "id": "ogQ3AqKFOe0V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configura el origen de los datos\n",
        "source_data_folder = \"/content/MisDatos\" # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "JVamUHaSPhYA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando y formateando el contenido necesario"
      ],
      "metadata": {
        "id": "_51IqCQD8tDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leyendo los PDFs del directorio configurado\n",
        "loader = PyPDFDirectoryLoader(source_data_folder)\n",
        "data_on_pdf = loader.load()\n",
        "# cantidad de data cargada\n",
        "len(data_on_pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MAEzY2EO8qn",
        "outputId": "023a6c3c-5328-4af3-c559-063e5032d3d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionando los datos. Con un tamaño delimitado (chunks) y 200 caracters de overlapping para preservar el contexto\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "splits = text_splitter.split_documents(data_on_pdf)\n",
        "# Cantidad de chunks obtenidos\n",
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z46WGUrmP56M",
        "outputId": "7ae0831b-b333-4f29-da2e-992e1c4ee446"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "embeddings_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "Oh5Z8PnWQi7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base de datos vectorial"
      ],
      "metadata": {
        "id": "N7zT2cT6ABGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configura el path a la base de datos\n",
        "path_db = \"/content/VectorDB\" # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "foSxJAayRKSL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenamos los chunks en la base de datos\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings_model, persist_directory=path_db)"
      ],
      "metadata": {
        "id": "ZyzgToxGQJgP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. RETRIEVAL"
      ],
      "metadata": {
        "id": "3AIv7ga7O5vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías necesarias"
      ],
      "metadata": {
        "id": "Xk06yy5EAXVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "iw9FfxuNSmvy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección / Configuración del LLM"
      ],
      "metadata": {
        "id": "BfVN830KAblv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Gemini"
      ],
      "metadata": {
        "id": "8mkN4FhYPTvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "e3RpHSOvKJgS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Gemma"
      ],
      "metadata": {
        "id": "3Mit2___PWLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get(\"KAGGLE_USERNAME\")\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get(\"KAGGLE_KEY\")\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # \"jax\" Or \"tensorflow\" or \"torch\".\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\""
      ],
      "metadata": {
        "id": "MIJjDhW2-RKW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install --upgrade keras>=3\n",
        "!pip install -q langchain langchain-google-vertexai"
      ],
      "metadata": {
        "id": "NrWQot_x-8Wt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Gemma using LangChain library\n",
        "from langchain_google_vertexai import GemmaChatLocalKaggle\n",
        "\n",
        "keras_backend: str = \"jax\"\n",
        "model_name = \"gemma_1.1_instruct_2b_en\"\n",
        "llm = GemmaChatLocalKaggle(\n",
        "    model_name=model_name,\n",
        "    model=model_name,\n",
        "    keras_backend=keras_backend,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "# If Error on command above, restart the session and run again."
      ],
      "metadata": {
        "id": "4F74zrPL-H3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d20ad7-0580-467e-8c9f-731cccde965f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'task.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.safetensors' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'model.safetensors.index.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_1.1_instruct_2b_en/3' to your Colab notebook...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "from langchain_core.output_parsers import BaseTransformOutputParser\n",
        "class GemmaOutputParser(BaseTransformOutputParser[str]):\n",
        "    \"\"\"OutputParser that parses LLM response and extract\n",
        "    the generated part.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def is_lc_serializable(cls) -> bool:\n",
        "        \"\"\"Return whether this class is serializable.\"\"\"\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        \"\"\"Return the output parser type for serialization.\"\"\"\n",
        "        return \"gemma_2_parser\"\n",
        "\n",
        "    def parse(self, text: str) -> str:\n",
        "        \"\"\"Return the input text with no changes.\"\"\"\n",
        "        model_start_token = \"model\\n\"\n",
        "        idx = text.rfind(model_start_token)\n",
        "        return text[idx + len(model_start_token) :] if idx > -1 else \"\"\n",
        "\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3H1CF505S7yA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del Prompt y Retriever\n",
        "\n"
      ],
      "metadata": {
        "id": "jLDhrOuOAny0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=6467f92b-9dac-5816-964f-8abcfa4e4456\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "PfAWLnX-RxiI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | GemmaOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "7hr9Uop5EaQl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecución del RAG"
      ],
      "metadata": {
        "id": "PlMQfYdgBEgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preguntas al Documento\n",
        "pregunta = \"Ingredientes del silpancho. \" # @param {type:\"string\"}\n",
        "response = rag_chain.invoke(pregunta )\n",
        "Markdown(response)\n"
      ],
      "metadata": {
        "id": "vrZao12WTLxt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "3d6e8856-6a6a-44f5-93cb-73b08e41093e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The ingredients for the silpancho are chopped onions, tomatoes, lettuce, and rice. The silpancho is prepared by sautéing the vegetables in oil until golden and then"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preguntas al Documento\n",
        "pregunta = \"Name of the dish\" # @param {type:\"string\"}\n",
        "response = rag_chain.invoke(pregunta )\n",
        "Markdown(response)\n"
      ],
      "metadata": {
        "id": "j9FhCZjcRIUg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "cf4a006b-6463-40d5-82e5-e16018cab1b0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The dish mentioned in the context is huevo duro, a traditional Bolivian traditional dish. It is a savory pancake made with cornmeal, eggs, and potatoes, and is often served with a side of salad."
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleanup\n",
        "# vectorstore.delete_collection()"
      ],
      "metadata": {
        "id": "UBtujO2XU9wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fuentes:**"
      ],
      "metadata": {
        "id": "yF9tJOT4TYV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dev.to/timesurgelabs/how-to-use-googles-gemini-pro-with-langchain-1eje\n",
        "\n",
        "# https://python.langchain.com/v0.2/docs/tutorials/rag/\n",
        "\n",
        "# https://smith.langchain.com/o/6467f92b-9dac-5816-964f-8abcfa4e4456/projects/p/d35fb5ce-7bac-4627-858c-621aa689239f?timeModel=%7B%22duration%22%3A%227d%22%7D"
      ],
      "metadata": {
        "id": "EJR2bsISNVkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}